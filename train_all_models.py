import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler
import joblib
import os
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Imports locaux
try:
    from data_loader import prepare_data_for_models, prepare_sequences_for_lstm
    from evaluation import evaluate_model_complete, find_optimal_threshold
    print("âœ… Modules locaux importÃ©s")
except ImportError:
    print("âš ï¸ Modules locaux non disponibles, dÃ©finition des fonctions de secours...")
    
    # Fonctions de secours
    def prepare_data_for_models():
        from data_loader import prepare_data_for_models as func
        return func()
    
    def prepare_sequences_for_lstm(X_train_normal, X_test, y_test, sequence_length=10):
        from data_loader import prepare_sequences_for_lstm as func
        return func(X_train_normal, X_test, y_test, sequence_length)

# Config
np.random.seed(42)
tf.random.set_seed(42)

print("="*70)
print("ğŸ¯ ENTRAÃNEMENT COMPLET - MAINTENANCE PRÃ‰DICTIVE")
print("="*70)

# CrÃ©er le dossier models s'il n'existe pas
os.makedirs('models', exist_ok=True)

# ==================== 1. CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES ====================

print("\nCHARGEMENT DES DONNÃ‰ES...")
try:
    # Utiliser use_all_features=True pour avoir 9 features
    X_train_normal, X_test, y_test, scaler = prepare_data_for_models(use_all_features=True)
    input_dim = X_train_normal.shape[1]
    
    print(f"âœ… DonnÃ©es chargÃ©es: Train={X_train_normal.shape}, Test={X_test.shape}")
    print(f"   Nombre de features: {input_dim} ")
    print(f"   Anomalies dans test: {y_test.sum()} ({y_test.sum()/len(y_test)*100:.1f}%)")
    
    # VÃ©rification
    if input_dim != 9:
        print(f"âš ï¸ ATTENTION: {input_dim} features au lieu de 9")
        print("   VÃ©rifiez data_loader.py")
    
except Exception as e:
    print(f"âŒ Erreur lors du chargement: {e}")
    raise

# Sauvegarder le scaler
joblib.dump(scaler, 'models/scaler.pkl')
print("âœ… Scaler sauvegardÃ©: models/scaler.pkl")

# ==================== 2. AUTOENCODEUR DENSE ====================
print("\n2ï¸âƒ£ ENTRAÃNEMENT AUTOENCODEUR DENSE...")

# Architecture
autoencoder_dense = Sequential([
    Input(shape=(input_dim,)),
    Dense(16, activation='relu'),
    Dense(8, activation='relu'),
    Dense(3, activation='relu', name='latent'),
    Dense(8, activation='relu'),
    Dense(16, activation='relu'),
    Dense(input_dim, activation='linear')
])
autoencoder_dense.compile(optimizer='adam', loss='mse')

# EntraÃ®nement
history_dense = autoencoder_dense.fit(
    X_train_normal, X_train_normal,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],
    verbose=1
)

# PrÃ©dictions
recon_test_dense = autoencoder_dense.predict(X_test, verbose=0)
mse_test_dense = np.mean(np.square(X_test - recon_test_dense), axis=1)

recon_train_dense = autoencoder_dense.predict(X_train_normal, verbose=0)
mse_train_dense = np.mean(np.square(X_train_normal - recon_train_dense), axis=1)

# Optimisation seuil
threshold_dense, f2_dense, _ = find_optimal_threshold(y_test, mse_test_dense, 'f2', 0.85)
y_pred_dense = (mse_test_dense > threshold_dense).astype(int)

# Ã‰valuation
results_dense = evaluate_model_complete(y_test, y_pred_dense, mse_test_dense, "Dense Autoencoder")

# Sauvegarde
autoencoder_dense.save('models/autoencoder_dense_model.h5')
print("âœ… Autoencodeur Dense sauvegardÃ©: models/autoencoder_dense_model.h5")

# ==================== 3. AUTOENCODEUR LSTM ====================
print("\n3ï¸âƒ£ ENTRAÃNEMENT AUTOENCODEUR LSTM...")

try:
    # PrÃ©parer sÃ©quences
    X_train_seq, X_test_seq, y_test_seq = prepare_sequences_for_lstm(
        X_train_normal, X_test, y_test, sequence_length=10
    )
    
    timesteps, n_features = X_train_seq.shape[1], X_train_seq.shape[2]
    
    # Architecture LSTM
    autoencoder_lstm = Sequential([
        Input(shape=(timesteps, n_features)),
        LSTM(32, activation='relu', return_sequences=True),
        LSTM(16, activation='relu', return_sequences=False),
        RepeatVector(timesteps),
        LSTM(16, activation='relu', return_sequences=True),
        LSTM(32, activation='relu', return_sequences=True),
        TimeDistributed(Dense(n_features))
    ])
    autoencoder_lstm.compile(optimizer='adam', loss='mse')
    
    # EntraÃ®nement
    history_lstm = autoencoder_lstm.fit(
        X_train_seq, X_train_seq,
        epochs=30,
        batch_size=32,
        validation_split=0.1,
        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],
        verbose=1
    )
    
    # PrÃ©dictions
    recon_test_lstm = autoencoder_lstm.predict(X_test_seq, verbose=0)
    mse_test_lstm = np.mean(np.square(X_test_seq - recon_test_lstm), axis=(1, 2))
    
    recon_train_lstm = autoencoder_lstm.predict(X_train_seq, verbose=0)
    mse_train_lstm = np.mean(np.square(X_train_seq - recon_train_lstm), axis=(1, 2))
    
    # Optimisation seuil
    threshold_lstm, f2_lstm, _ = find_optimal_threshold(y_test_seq, mse_test_lstm, 'f2', 0.85)
    y_pred_lstm = (mse_test_lstm > threshold_lstm).astype(int)
    
    # Ã‰valuation
    results_lstm = evaluate_model_complete(y_test_seq, y_pred_lstm, mse_test_lstm, "LSTM Autoencoder")
    
    # Sauvegarde
    autoencoder_lstm.save('models/autoencoder_lstm_model.h5')
    print("âœ… Autoencodeur LSTM sauvegardÃ©: models/autoencoder_lstm_model.h5")
    
except Exception as e:
    print(f"âš ï¸ Erreur lors de l'entraÃ®nement LSTM: {e}")
    results_lstm = None
    threshold_lstm = None

# ==================== 4. MÃ‰THODES CLASSIQUES ====================
print("\nENTRAÃNEMENT MÃ‰THODES CLASSIQUES...")

# Contamination estimÃ©e (proportion d'anomalies)
contamination = y_test.sum() / len(y_test)

# 4.1 Isolation Forest
print("   Isolation Forest...")
iso_forest = IsolationForest(
    contamination=contamination, 
    random_state=42,
    n_estimators=100
)
iso_forest.fit(X_train_normal)

y_scores_iso = -iso_forest.score_samples(X_test)  # Plus Ã©levÃ© = plus anormal
y_pred_iso = iso_forest.predict(X_test)
y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # Convertir: -1=anomalie -> 1

results_iso = evaluate_model_complete(y_test, y_pred_iso, y_scores_iso, "Isolation Forest")
joblib.dump(iso_forest, 'models/isolation_forest.pkl')
print("   âœ… Isolation Forest sauvegardÃ©")

# 4.2 One-Class SVM
print("   One-Class SVM...")
oc_svm = OneClassSVM(nu=contamination, kernel='rbf', gamma='scale')
oc_svm.fit(X_train_normal)

y_scores_svm = -oc_svm.decision_function(X_test)  # Plus Ã©levÃ© = plus anormal
y_pred_svm = oc_svm.predict(X_test)
y_pred_svm = np.where(y_pred_svm == -1, 1, 0)  # Convertir: -1=anomalie -> 1

results_svm = evaluate_model_complete(y_test, y_pred_svm, y_scores_svm, "One-Class SVM")
joblib.dump(oc_svm, 'models/one_class_svm.pkl')
print("   âœ… One-Class SVM sauvegardÃ©")

# 4.3 Local Outlier Factor (LOF)
print("   Local Outlier Factor...")
lof = LocalOutlierFactor(
    n_neighbors=20, 
    contamination=contamination,
    novelty=True
)
lof.fit(X_train_normal)

y_scores_lof = -lof.decision_function(X_test)  # Plus Ã©levÃ© = plus anormal
y_pred_lof = lof.predict(X_test)
y_pred_lof = np.where(y_pred_lof == -1, 1, 0)  # Convertir: -1=anomalie -> 1

results_lof = evaluate_model_complete(y_test, y_pred_lof, y_scores_lof, "LOF")
joblib.dump(lof, 'models/lof.pkl')
print("   âœ… LOF sauvegardÃ©")

# ==================== 5. SAUVEGARDE DES PARAMÃˆTRES ====================
print("\n5ï¸âƒ£ SAUVEGARDE DES PARAMÃˆTRES...")

params = {
    'threshold_dense': float(threshold_dense),
    'threshold_lstm': float(threshold_lstm) if threshold_lstm else 0.1,
    'input_dim': int(input_dim),
    'sequence_length': 10,
    'contamination': float(contamination),
    'features': ['Air temperature', 'Process temperature', 
                'Rotational speed', 'Torque', 'Tool wear']
}

joblib.dump(params, 'models/model_parameters.pkl')
print("âœ… ParamÃ¨tres sauvegardÃ©s: models/model_parameters.pkl")

# ==================== 6. CRÃ‰ATION DU FICHIER DE RÃ‰SULTATS ====================
print("\n6ï¸âƒ£ CRÃ‰ATION DU FICHIER DE COMPARAISON...")

# Rassembler tous les rÃ©sultats
all_results = []
for results in [results_dense, results_lstm, results_iso, results_svm, results_lof]:
    if results is not None:
        all_results.append(results)

# CrÃ©er DataFrame
if all_results:
    results_df = pd.DataFrame(all_results)
    
    # Trier par F2-Score (mÃ©trique principale)
    results_df = results_df.sort_values('F2-Score', ascending=False)
    
    # Sauvegarder
    results_df.to_csv('model_comparison_results.csv', index=False)
    
    print("âœ… RÃ©sultats sauvegardÃ©s: model_comparison_results.csv")
    
    # Afficher le classement
    print("\n" + "="*70)
    print("ğŸ† CLASSEMENT DES MODÃˆLES (par F2-Score):")
    print("="*70)
    for i, row in results_df.iterrows():
        print(f"{i+1:2d}. {row['Model']:25s} | F2: {row['F2-Score']:.4f} | "
              f"Recall: {row['Recall']:.4f} | AUC-PR: {row['AUC-PR']:.4f}")
    
    # Meilleur modÃ¨le
    best_model = results_df.iloc[0]
    print(f"\nğŸ¥‡ MEILLEUR MODÃˆLE: {best_model['Model']}")
    print(f"   F2-Score: {best_model['F2-Score']:.4f}")
    print(f"   Recall: {best_model['Recall']:.4f} ({best_model['Recall']*100:.1f}%)")
    print(f"   PrÃ©cision: {best_model['Precision']:.4f}")
    
    if best_model['Recall'] >= 0.85:
        print("   âœ… Objectif de recall (85%) atteint!")
    else:
        print(f"   âš ï¸ Recall infÃ©rieur Ã  l'objectif de 85%")
else:
    print("âš ï¸ Aucun rÃ©sultat disponible pour la comparaison")

# ==================== 7. GÃ‰NÃ‰RATION DES VISUALISATIONS ====================
print("\nGÃ‰NÃ‰RATION DES VISUALISATIONS...")

try:
    from evaluation import plot_roc_pr_comparison, plot_reconstruction_error_distribution
    
    # DonnÃ©es pour les courbes
    results_dict = {
        'Dense Autoencoder': results_dense,
        'LSTM Autoencoder': results_lstm if results_lstm else None,
        'Isolation Forest': results_iso,
        'One-Class SVM': results_svm,
        'LOF': results_lof
    }
    
    y_true_dict = {
        'Dense Autoencoder': y_test,
        'LSTM Autoencoder': y_test_seq if results_lstm else None,
        'Isolation Forest': y_test,
        'One-Class SVM': y_test,
        'LOF': y_test
    }
    
    scores_dict = {
        'Dense Autoencoder': mse_test_dense,
        'LSTM Autoencoder': mse_test_lstm if results_lstm else None,
        'Isolation Forest': y_scores_iso,
        'One-Class SVM': y_scores_svm,
        'LOF': y_scores_lof
    }
    
    # Filtrer les modÃ¨les valides
    valid_models = {k: v for k, v in results_dict.items() if v is not None}
    valid_y_true = {k: v for k, v in y_true_dict.items() if v is not None}
    valid_scores = {k: v for k, v in scores_dict.items() if v is not None}
    
    if len(valid_models) > 1:
        # Courbes ROC et PR
        plot_roc_pr_comparison(valid_models, valid_y_true, valid_scores, save_dir='reports/figures/')
        
        # Distribution des erreurs pour Dense AE
        plot_reconstruction_error_distribution(
            mse_test_dense, y_test, threshold_dense,
            "Dense Autoencoder", save_path='reports/figures/reconstruction_errors_dense.png'
        )
        
        print("âœ… Visualisations gÃ©nÃ©rÃ©es")
    else:
        print("âš ï¸ Pas assez de modÃ¨les valides pour les visualisations")
        
except Exception as e:
    print(f"âš ï¸ Erreur lors de la gÃ©nÃ©ration des visualisations: {e}")

# ==================== 8. RÃ‰SUMÃ‰ FINAL ====================
print("\n" + "="*70)
print("âœ… ENTRAÃNEMENT TERMINÃ‰ AVEC SUCCÃˆS!")
print("="*70)
print("\nğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S :")
print("models/")
print("  â”œâ”€â”€ autoencoder_dense_model.h5")
print("  â”œâ”€â”€ autoencoder_lstm_model.h5")
print("  â”œâ”€â”€ isolation_forest.pkl")
print("  â”œâ”€â”€ one_class_svm.pkl")
print("  â”œâ”€â”€ lof.pkl")
print("  â”œâ”€â”€ model_parameters.pkl")
print("  â””â”€â”€ scaler.pkl")
print("\nğŸ“Š FICHIERS DE RÃ‰SULTATS :")
print("â”œâ”€â”€ model_comparison_results.csv")
print("â”œâ”€â”€ comparison_roc_curves.png")
print("â””â”€â”€ reconstruction_errors_dense.png")


# Sauvegarder aussi les rÃ©sultats au format JSON pour l'application
try:
    results_df.to_json('models/results_summary.json', orient='records', indent=2)
    print("âœ… RÃ©sultats supplÃ©mentaires sauvegardÃ©s au format JSON")
except:
    pass